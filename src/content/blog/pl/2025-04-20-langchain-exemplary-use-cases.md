---
author: Daniel Gustaw
canonicalName: langchain-exemplary-use-cases
coverImage: https://ucarecdn.com/f618a5e8-e72d-4c62-920f-3ee29b98f8c9/-/preview/640x360/
description: Ekstrakcja uporzÄ…dkowanych danych z faktur PDF i budowanie elastycznego systemu wyszukiwania dokumentÃ³w z LangChain i FAISS.
excerpt: Ekstrakcja uporzÄ…dkowanych danych z faktur PDF i budowanie elastycznego systemu wyszukiwania dokumentÃ³w z LangChain i FAISS.
publishDate: 2025-04-20 00:00:00+00:00
slug: pl/langchain-przykladowe-przypadki-uzycia
tags:
- langchain
- openai
- rag
title: PrzykÅ‚ady zastosowania LangChain
updateDate: 2025-04-20 00:00:00+00:00
---

# W tym poÅ›cie na blogu zobaczymy 2 przykÅ‚ady:
- Ekstrakcja danych strukturalnych z PDF
- RAG (Odpowiadanie na pytania) z LangChain

## Ekstrakcja danych strukturalnych z faktur PDF przy uÅ¼yciu LangChain

Cyfryzacja faktur to powszechne zadanie w automatyzacji biznesu - szczegÃ³lnie gdy sÄ… one przesyÅ‚ane jako zeskanowane pliki PDF. W tym poÅ›cie pokaÅ¼emy, jak uÅ¼ywaÄ‡ LangChain, 
OpenAI i Pydantic do ekstrakcji danych strukturalnych, takich jak nazwisko klienta, data, numer faktury oraz caÅ‚kowita wartoÅ›Ä‡ z plikÃ³w PDF.

Zanurzmy siÄ™ w to! ğŸš€

### ğŸ’¡ Cel

Chcemy zbudowaÄ‡ skrypt, ktÃ³ry akceptuje:

- FakturÄ™ PDF
- Model danych opisujÄ…cy oczekiwane pola

I zwraca:

- Strukturalny wynik JSON z wyekstrahowanymi wartoÅ›ciami

MoÅ¼e to byÄ‡ bardzo przydatne w scenariuszach takich jak:

- Automatyzacja ksiÄ™gowoÅ›ci
- Integracja faktur z CRM lub ERP
- ZastÄ™powanie rÄ™cznego wprowadzania danych

UÅ¼yjemy nastÄ™pujÄ…cych pakietÃ³w:

```bash
pip install langchain openai pydantic pdfplumber langchain-community
```

A oto faktura:

<embed src="https://fra1.digitaloceanspaces.com/preciselab/blog/doc/invoice1.pdf" width="800" height="1180" type="application/pdf">


### ğŸ§± Krok 1: Zdefiniuj Model Danych

UÅ¼yjemy Pydantic, aby zdefiniowaÄ‡, co chcemy wyciÄ…gnÄ…Ä‡ z kaÅ¼dej faktury.

```python
from pydantic import BaseModel

class InvoiceData(BaseModel):
    client: str
    date: str
    invoice_number: str
    total_amount: str
```

### ğŸ“„ Krok 2: Wydobywanie tekstu z PDF

UÅ¼yjemy pdfplumber, aby uzyskaÄ‡ zawartoÅ›Ä‡ tekstowÄ… z PDF:

```python
import pdfplumber

def extract_text_from_pdf(path):
    with pdfplumber.open(path) as pdf:
        return "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
```

### ğŸ§  Krok 3: Skonfiguruj LangChain do Strukturacji Odpowiedzi

LangChain + OpenAI + podpowiedÅº + model danych = magia âœ¨

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
```

UÅ¼yjemy podpowiedzi, aby powiedzieÄ‡ modelowi, czego chcemy:

```python
template = """
Extract the following information from the text below:
- Client name
- Date
- Invoice number
- Total value for payment

{format_instructions}

Text:
{text}
"""
```

### ğŸ”— Krok 4: StwÃ³rz Å‚aÅ„cuch i parsuj

ÅÄ…czymy podpowiedÅº, model i parser:

```python
from langchain_core.runnables import Runnable

def parse(invoice_path):
    llm = ChatOpenAI(temperature=0)
    parser = PydanticOutputParser(pydantic_object=InvoiceData)

    prompt = PromptTemplate(
        input_variables=["text"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
        template=template
    )

    chain = prompt | llm | parser

    pdf_text = extract_text_from_pdf(invoice_path)
    return chain.invoke({"text": pdf_text})
```

### ğŸ§ª Krok 5: Uruchom i wydrukuj jako JSON

Na koniec uruchamiamy peÅ‚ny proces i drukujemy wynik:

```python
import json

if __name__ == '__main__':
    result = parse("invoice1.pdf")
    print(json.dumps(result.dict(), indent=2))
```

### âœ… PrzykÅ‚adowy Wynik

```json
{
  "client": "Test Business",
  "date": "January 25, 2016",
  "invoice_number": "INV-3337",
  "total_amount": "$93.50"
}
```

# Budowanie Elastycznego Systemu Wyszukiwania DokumentÃ³w z Langchain i FAISS

Teraz przejdziemy do budowy elastycznego systemu wyszukiwania dokumentÃ³w z uÅ¼yciem Langchain, FAISS i osadzeÅ„ OpenAI. Naszym celem jest stworzenie systemu, ktÃ³ry pozwala na efektywne przeszukiwanie plikÃ³w PDF, bez twardego kodowania nazw plikÃ³w, oraz Å‚atwego rozszerzania na rÃ³Å¼ne przypadki uÅ¼ycia.

Zanurzmy siÄ™ w to, jak moÅ¼emy zbudowaÄ‡ taki system, utrzymujÄ…c kod czysty i skalowalny.

### Czego siÄ™ Nauczysz
- Jak uÅ¼ywaÄ‡ Langchain do Å‚adowania i indeksowania dokumentÃ³w PDF.
- Jak stworzyÄ‡ system QA oparty na wyszukiwaniu przy uÅ¼yciu FAISS i osadzeÅ„ OpenAI.
- Jak przeksztaÅ‚ciÄ‡ kod, aby staÅ‚ siÄ™ elastyczny i moduÅ‚owy.
- Jak dynamicznie przeszukiwaÄ‡ swoje dokumenty, przekazujÄ…c nazwÄ™ pliku i pytanie w czasie rzeczywistym.

### Dlaczego UÅ¼ywaÄ‡ Langchain i FAISS?

Zanim przejdziemy do kodu, krÃ³tko omÃ³wmy, dlaczego uÅ¼ywamy Langchain i FAISS do tego zadania.

- **Langchain**: UÅ‚atwia budowanie aplikacji wokÃ³Å‚ LLM (DuÅ¼e Modele JÄ™zykowe) poprzez dostarczanie potÄ™Å¼nych narzÄ™dzi do Å‚adowania dokumentÃ³w, dzielenia tekstu, osadzeÅ„ i wielu innych.
- **FAISS**: Wyszukiwanie PodobieÅ„stwa AI Facebooka (FAISS) to biblioteka do efektywnego wyszukiwania podobieÅ„stw i grupowania gÄ™stych wektorÃ³w. UmoÅ¼liwia nam szybkie znajdowanie odpowiednich fragmentÃ³w z duÅ¼ych zbiorÃ³w dokumentÃ³w na podstawie osadzeÅ„.

Konfiguracja Projektu

Zacznijmy od zainstalowania niezbÄ™dnych pakietÃ³w. MoÅ¼esz je zainstalowaÄ‡ za pomocÄ… pip:

```python
pip install langchain langchain-openai faiss-cpu
```

Aby skorzystaÄ‡ z ich modelu osadzania, potrzebujesz rÃ³wnieÅ¼ klucza API z OpenAI.

### Kluczowe pojÄ™cia w kodzie

Zanim przejdziemy do kodu, rozÅ‚Ã³Å¼my go na czÄ™Å›ci:

- Åadowanie dokumentu: UÅ¼ywamy PyPDFLoader do Å‚adowania zawartoÅ›ci PDF.
- Dzielenie tekstu: DuÅ¼e pliki PDF mogÄ… zawieraÄ‡ zbyt wiele tekstu do przetworzenia w jednym kroku. UÅ¼ywamy RecursiveCharacterTextSplitter, aby podzieliÄ‡ tekst na mniejsze fragmenty.
- Osadzenia: UÅ¼ywamy osadzeÅ„ OpenAI (OpenAIEmbeddings), aby przeksztaÅ‚ciÄ‡ tekst w reprezentacje wektorowe.
- Indeks FAISS: Przechowuje i zarzÄ…dza wektoryzowanym tekstem, umoÅ¼liwiajÄ…c szybkie wyszukiwanie podobieÅ„stw.
- QA wyszukiwania: Kluczowa funkcjonalnoÅ›Ä‡ pozwala nam zadawaÄ‡ pytania w odniesieniu do zindeksowanego dokumentu, uÅ¼ywajÄ…c Å‚aÅ„cucha RetrievalQA.

Teraz zanurzmy siÄ™ w sam kod.

### Refaktoryzowany kod

Oto refaktoryzowany kod, ktÃ³ry to osiÄ…ga:

```python
import os
import logging
from typing import Optional

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

INDEX_PATH = "faiss_book_index"

def load_or_create_vectorstore(pdf_path: str, embeddings: OpenAIEmbeddings) -> FAISS:
    """Load FAISS index if it exists; otherwise create it from the PDF."""
    if os.path.exists(INDEX_PATH):
        logger.info("Loading existing FAISS index...")
        return FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)

    logger.info("Creating new FAISS index from PDF...")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    doc_chunks = text_splitter.split_documents(docs)

    vectorstore = FAISS.from_documents(doc_chunks, embeddings)
    vectorstore.save_local(INDEX_PATH)
    return vectorstore

def parse(pdf_path: str, question: str) -> str:
    """Process PDF and ask question using retrieval QA."""
    embeddings = OpenAIEmbeddings()
    vectorstore = load_or_create_vectorstore(pdf_path, embeddings)

    retriever = vectorstore.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4-turbo"),
        retriever=retriever
    )

    response = qa_chain.invoke(question)
    return response["result"]

if __name__ == '__main__':
    question = 'ALEKSANDER FREDRO w utworze "Zemsta" umieÅ›ciÅ‚ werset: "jeÅ›li nie chcesz mojej zguby, ___ daj mi luby", czym jest ___? Co luby ma daÄ‡ osobie wypowiadajÄ…cej ten cytat?'
    pdf_file = "zemsta.pdf"
    answer = parse(pdf_file, question)
    print(answer)
```

### Analiza kodu

```python
load_or_create_vectorstore(pdf_path, embeddings):
```

Ta funkcja sprawdza, czy istnieje istniejÄ…cy indeks FAISS. JeÅ›li tak, Å‚aduje go. W przeciwnym razie przetwarza plik PDF, dzieli go na mniejsze fragmenty i tworzy nowy indeks FAISS.

```python
parse(pdf_path, question):
```

To jest punkt wejÅ›cia, w ktÃ³rym moÅ¼esz podaÄ‡ dowolnÄ… Å›cieÅ¼kÄ™ do pliku PDF oraz pytanie. Åadowany jest dokument, pobierane sÄ… odpowiednie informacje, a pytanie jest zadawane za pomocÄ… Å‚aÅ„cucha RetrievalQA wspieranego przez OpenAI.

```python
main():
```

W bloku __main__ okreÅ›lamy dokument i pytanie dynamicznie. MoÅ¼esz zmieniÄ‡ nazwÄ™ pliku i pytanie w czasie rzeczywistym, co sprawia, Å¼e ten kod jest elastyczny dla rÃ³Å¼nych zastosowaÅ„.

### Odpowiedzi

W tej chwili czaty dostÄ™pne w przeglÄ…darkach, takie jak chatgpt, majÄ… 
moÅ¼liwoÅ›Ä‡ odpowiadania na pytania w oparciu o kontekst dokumentu znalezionego w internecie. 
Dzieje siÄ™ to dziÄ™ki wykorzystaniu osadzeÅ„ i magazynÃ³w wektorowych do znajdowania odpowiednich fragmentÃ³w 
w dokumencie.

WiÄ™c jeÅ›li zadacie pytanie o dobrze znany dokument, jak ksiÄ…Å¼ka "Zemsta" z naszego przykÅ‚adu, otrzymacie poprawnÄ… odpowiedÅº, ale jeszcze niedawno czaty internetowe odpowiadaÅ‚y bez dostÄ™pu do ÅºrÃ³deÅ‚, a jakoÅ›Ä‡ odpowiedzi byÅ‚a znacznie gorsza.

Prezentowana technika nadal jest przydatna dla dokumentÃ³w, ktÃ³re nie sÄ… dostÄ™pne publicznie.

### Wnioski

W tym poÅ›cie zbudowaliÅ›my elastyczny system wyszukiwania dokumentÃ³w przy uÅ¼yciu Langchain, FAISS i osadzeÅ„ OpenAI. ZrefaktoryzowaliÅ›my kod, aby byÅ‚ bardziej modularny i dynamiczny, co umoÅ¼liwia wykonywanie zapytaÅ„ do dowolnego dokumentu PDF.

To podejÅ›cie moÅ¼na rozszerzyÄ‡ na wiele innych zastosowaÅ„, takich jak zapytania do rÃ³Å¼nych formatÃ³w dokumentÃ³w, stosowanie bardziej zaawansowanych technik NLP lub skalowanie do wiÄ™kszego korpusu dokumentÃ³w. Nie ma granic!
