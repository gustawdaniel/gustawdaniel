---
canonicalName: ai-voices
---

## Text to Voice

Prepare the environment

```bash
conda create -n fish-speech python=3.10
conda activate fish-speech
```

Install cli to download models from huggingface hub

```bash
pip install -U "huggingface_hub[cli]"
```

Download fish-speech-1.4 model

```bash
huggingface-cli download fishaudio/fish-speech-1.4 --local-dir checkpoints/fish-speech-1.4/
```

Install dependencies in the root of the project

```bash
pip3 install -e .
```

Generate codes_0.npy file from text

```bash
python tools/llama/generate.py --text "Your text here" --checkpoint-path "checkpoints/fish-speech-1.4"
```

Generate audio from codes_0.npy file

```
python tools/vqgan/inference.py -i "codes_0.npy" --checkpoint-path "checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth"
```

## Text to video

Prepare environment

```bash
conda create -n cog-video python=3.10
conda activate cog-video
```

Install dependencies
 
```bash
pip install diffusers transformers hf_transfer
pip install accelerate==0.33.0
pip install sentencepiece
```

Set

```
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

to avoid errors like this:

> torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 5.70 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 4.22 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 419.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Run `main.py` script


To fix

```
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version
Traceback (most recent call last):
  File "/home/daniel/exp/cog-video/main.py", line 45, in <module>
    export_to_video(video, "output.mp4", fps=8)
  File "/home/daniel/miniconda3/envs/cog-video/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 154, in export_to_video
    return _legacy_export_to_video(video_frames, output_video_path, fps)
  File "/home/daniel/miniconda3/envs/cog-video/lib/python3.10/site-packages/diffusers/utils/export_utils.py", line 121, in _legacy_export_to_video
    raise ImportError(BACKENDS_MAPPING["opencv"][1].format("export_to_video"))
ImportError: 
export_to_video requires the OpenCV library but it was not found in your environment. You can install it with pip: `pip
install opencv-python`
```

install

```bash
pip install imageio imageio-ffmpeg opencv-python
```

```python
import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

import torch
from diffusers import AutoencoderKLCogVideoX, CogVideoXPipeline, CogVideoXTransformer3DModel
from diffusers.utils import export_to_video
from transformers import T5EncoderModel

# Models: "THUDM/CogVideoX-2b" or "THUDM/CogVideoX-5b"
model_id = "THUDM/CogVideoX-5b"

# Thank you [@camenduru](https://github.com/camenduru)!
# The reason for using checkpoints hosted by Camenduru instead of the original is because they exported
# with a max_shard_size of "5GB" when saving the model with `.save_pretrained`. The original converted
# model was saved with "10GB" as the max shard size, which causes the Colab CPU RAM to be insufficient
# leading to OOM (on the CPU)

transformer = CogVideoXTransformer3DModel.from_pretrained("camenduru/cogvideox-5b-float16", subfolder="transformer", torch_dtype=torch.float16)
text_encoder = T5EncoderModel.from_pretrained("camenduru/cogvideox-5b-float16", subfolder="text_encoder", torch_dtype=torch.float16)
vae = AutoencoderKLCogVideoX.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float16)

# Create pipeline and run inference
pipe = CogVideoXPipeline.from_pretrained(
    model_id,
    text_encoder=text_encoder,
    transformer=transformer,
    vae=vae,
    torch_dtype=torch.float16,
)

pipe.enable_sequential_cpu_offload()
# pipe.vae.enable_tiling()

prompt = (
    "A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. "
    "The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other "
    "pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, "
    "casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. "
    "The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical "
    "atmosphere of this unique musical performance."
)

video = pipe(prompt=prompt, guidance_scale=6, use_dynamic_cfg=True, num_inference_steps=50).frames[0]

export_to_video(video, "output.mp4", fps=8)
```


